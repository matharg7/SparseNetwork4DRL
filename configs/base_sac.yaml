##################################################################################
# Common
##################################################################################

project_name: 'StaticSparseNetwork'
exp_name: 'test'
seed: 0
server: 'local'

env_name: 'humanoid-walk'
##################################################################################
# Training
##################################################################################
actor_num_blocks: 1
actor_hidden_dim: 128

critic_num_blocks: 2
critic_hidden_dim: 512

actor_pruner: "magnitude"
actor_sparsity: 0.95
actor_update_frequency: 1500
actor_start_step: 175000
actor_end_step: 625000
actor_sparsity_distribution: "erk"
actor_drop_fraction: 0.5

critic_pruner: "magnitude"
critic_sparsity: 0.95
critic_update_frequency: 1500
critic_start_step: 175000
critic_end_step: 625000
critic_sparsity_distribution: "erk"
critic_drop_fraction: 0.5

# gamma value is set with a heuristic from TD-MPCv2
eff_episode_len: ${eval:'${env.max_episode_steps} / ${env.action_repeat}'}
gamma: ${eval:'max(min((${eff_episode_len} / 5 - 1) / (${eff_episode_len} / 5), 0.995), 0.95)'}
n_step: 1

num_train_envs: ${env.num_train_envs}
num_env_steps: ${env.num_env_steps}
action_repeat: ${env.action_repeat}

num_interaction_steps: ${eval:'${num_env_steps} / (${num_train_envs} * ${action_repeat})'}
updates_per_interaction_step: 2           # number of updates per interaction step.
evaluation_per_interaction_step: 5_000   # evaluation frequency per interaction step.
recording_per_interaction_step: ${num_interaction_steps}   # video recording frequency per interaction step.
logging_per_interaction_step: 1_000       # logging frequency per interaction step.
num_eval_episodes: 10
num_record_episodes: 1

defaults:
- _self_
- agent: sac_simba
- buffer: numpy_uniform
- env: dmc_hard